{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# LLM Health Advice Evaluation Analysis\n",
    "\n",
    "**Research Question:** How factually accurate and unbiased are GPT-4's responses to common mental health and wellness questions compared to information from the CDC and WHO?\n",
    "\n",
    "## Project Overview\n",
    "- **13 health questions** spanning pregnancy, mental health, general wellness, and medical procedures\n",
    "- **GPT-4 responses** generated for each question  \n",
    "- **Ground truth** from verified sources (CDC, WHO, Mayo Clinic, medical communities)\n",
    "- **Expert evaluations** using GPT-4 as evaluator on 4 criteria: Factual Accuracy, Clarity, Neutrality, Helpfulness\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import re\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set styling\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"üìä Libraries loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_path = Path(\"../data\")\n",
    "\n",
    "with open(data_path / \"questions.json\") as f:\n",
    "    questions_data = json.load(f)\n",
    "\n",
    "with open(data_path / \"evaluations.json\") as f:\n",
    "    evaluations_data = json.load(f)\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(questions_data)} questions\")\n",
    "print(f\"‚úÖ Loaded {len(evaluations_data)} evaluations\")\n",
    "\n",
    "# Display first question as example\n",
    "print(\"\\nüìã Example Question:\")\n",
    "print(f\"ID: {questions_data[0]['id']}\")\n",
    "print(f\"Question: {questions_data[0]['question']}\")\n",
    "print(f\"GPT Response: {questions_data[0]['response'][:100]}...\")\n",
    "print(f\"Ground Truth: {questions_data[0]['answer'][:100]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_evaluation_scores(evaluation_text):\n",
    "    \"\"\"Extract numerical scores from evaluation text\"\"\"\n",
    "    scores = {}\n",
    "    \n",
    "    # Look for patterns like \"Factual Accuracy: 4\" or \"Accuracy: 4\"\n",
    "    patterns = {\n",
    "        'factual_accuracy': r'(?:Factual\\s+)?Accuracy:\\s*(\\d+)',\n",
    "        'clarity': r'Clarity:\\s*(\\d+)',\n",
    "        'neutrality': r'Neutrality:\\s*(\\d+)',\n",
    "        'helpfulness': r'Helpfulness:\\s*(\\d+)'\n",
    "    }\n",
    "    \n",
    "    for criterion, pattern in patterns.items():\n",
    "        match = re.search(pattern, evaluation_text, re.IGNORECASE)\n",
    "        if match:\n",
    "            scores[criterion] = int(match.group(1))\n",
    "        else:\n",
    "            scores[criterion] = None\n",
    "    \n",
    "    return scores\n",
    "\n",
    "# Test the function with first evaluation\n",
    "test_eval = evaluations_data[0]['evaluation']\n",
    "print(\"üîç Testing score extraction:\")\n",
    "print(f\"Evaluation text: {test_eval[:200]}...\")\n",
    "print(f\"Extracted scores: {parse_evaluation_scores(test_eval)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive analysis dataframe\n",
    "analysis_data = []\n",
    "\n",
    "for eval_item in evaluations_data:\n",
    "    # Parse evaluation scores\n",
    "    scores = parse_evaluation_scores(eval_item['evaluation'])\n",
    "    \n",
    "    # Categorize question type\n",
    "    question = eval_item['question'].lower()\n",
    "    if any(word in question for word in ['pregnant', 'pregnancy', 'birth', 'c section', 'uterus']):\n",
    "        category = 'Pregnancy/Reproductive Health'\n",
    "    elif any(word in question for word in ['psychologist', 'meds', 'medication', 'antidepressant']):\n",
    "        category = 'Mental Health'\n",
    "    elif any(word in question for word in ['surgery', 'mri', 'iud', 'doctor', 'medical', 'procedure']):\n",
    "        category = 'Medical Procedures'\n",
    "    else:\n",
    "        category = 'General Health'\n",
    "    \n",
    "    # Calculate response lengths\n",
    "    gpt_response_length = len(eval_item['gpt_response'].split())\n",
    "    ground_truth_length = len(eval_item['ground_truth'].split())\n",
    "    \n",
    "    analysis_data.append({\n",
    "        'id': eval_item['id'],\n",
    "        'question': eval_item['question'][:100] + '...' if len(eval_item['question']) > 100 else eval_item['question'],\n",
    "        'category': category,\n",
    "        'factual_accuracy': scores['factual_accuracy'],\n",
    "        'clarity': scores['clarity'],\n",
    "        'neutrality': scores['neutrality'],\n",
    "        'helpfulness': scores['helpfulness'],\n",
    "        'gpt_response_length': gpt_response_length,\n",
    "        'ground_truth_length': ground_truth_length,\n",
    "        'evaluation_text': eval_item['evaluation']\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(analysis_data)\n",
    "\n",
    "# Calculate overall score\n",
    "df['overall_score'] = df[['factual_accuracy', 'clarity', 'neutrality', 'helpfulness']].mean(axis=1)\n",
    "\n",
    "print(f\"üìä Created analysis dataframe with {len(df)} entries\")\n",
    "print(f\"Categories: {df['category'].value_counts().to_dict()}\")\n",
    "print(f\"\\nüéØ Overall Statistics:\")\n",
    "print(df[['factual_accuracy', 'clarity', 'neutrality', 'helpfulness', 'overall_score']].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## üìä Key Findings Visualizations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Overall Performance Across Criteria\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Criteria scores distribution\n",
    "criteria = ['factual_accuracy', 'clarity', 'neutrality', 'helpfulness']\n",
    "mean_scores = [df[criterion].mean() for criterion in criteria]\n",
    "criteria_labels = ['Factual\\nAccuracy', 'Clarity', 'Neutrality', 'Helpfulness']\n",
    "\n",
    "ax1.bar(criteria_labels, mean_scores, color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4'])\n",
    "ax1.set_ylim(0, 5)\n",
    "ax1.set_title('Average Scores by Criteria', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylabel('Score (1-5)')\n",
    "for i, v in enumerate(mean_scores):\n",
    "    ax1.text(i, v + 0.1, f'{v:.2f}', ha='center', fontweight='bold')\n",
    "\n",
    "# Score distribution histogram\n",
    "ax2.hist(df['overall_score'], bins=10, alpha=0.7, color='#FF6B6B', edgecolor='black')\n",
    "ax2.set_title('Distribution of Overall Scores', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Overall Score (1-5)')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.axvline(df['overall_score'].mean(), color='red', linestyle='--', linewidth=2, \n",
    "           label=f'Mean: {df[\"overall_score\"].mean():.2f}')\n",
    "ax2.legend()\n",
    "\n",
    "# Performance by category\n",
    "category_scores = df.groupby('category')['overall_score'].mean().sort_values(ascending=True)\n",
    "ax3.barh(category_scores.index, category_scores.values, color='#45B7D1')\n",
    "ax3.set_title('Average Score by Question Category', fontsize=14, fontweight='bold')\n",
    "ax3.set_xlabel('Average Overall Score')\n",
    "for i, v in enumerate(category_scores.values):\n",
    "    ax3.text(v + 0.05, i, f'{v:.2f}', va='center', fontweight='bold')\n",
    "\n",
    "# Individual question performance\n",
    "question_performance = df[['question', 'overall_score']].sort_values('overall_score')\n",
    "top_5 = question_performance.tail(5)\n",
    "bottom_5 = question_performance.head(5)\n",
    "\n",
    "y_pos = range(len(top_5))\n",
    "ax4.barh(y_pos, top_5['overall_score'], color='green', alpha=0.7, label='Top 5')\n",
    "ax4.set_yticks(y_pos)\n",
    "ax4.set_yticklabels([q[:30] + '...' for q in top_5['question']], fontsize=9)\n",
    "ax4.set_title('Top 5 Performing Questions', fontsize=14, fontweight='bold')\n",
    "ax4.set_xlabel('Overall Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/overall_performance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"üéØ KEY INSIGHTS:\")\n",
    "print(f\"‚Ä¢ Average Overall Score: {df['overall_score'].mean():.2f}/5.0\")\n",
    "print(f\"‚Ä¢ Highest Criterion: {criteria_labels[np.argmax(mean_scores)]} ({max(mean_scores):.2f})\")\n",
    "print(f\"‚Ä¢ Lowest Criterion: {criteria_labels[np.argmin(mean_scores)]} ({min(mean_scores):.2f})\")\n",
    "print(f\"‚Ä¢ Best Category: {category_scores.index[-1]} ({category_scores.iloc[-1]:.2f})\")\n",
    "print(f\"‚Ä¢ Worst Category: {category_scores.index[0]} ({category_scores.iloc[0]:.2f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Detailed Analysis: Potential Issues and Bias Detection\n",
    "print(\"üîç DETAILED ANALYSIS:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Find questions with concerning scores\n",
    "low_accuracy = df[df['factual_accuracy'] <= 2]\n",
    "if not low_accuracy.empty:\n",
    "    print(f\"\\n‚ùå LOW FACTUAL ACCURACY QUESTIONS ({len(low_accuracy)}):\")\n",
    "    for _, row in low_accuracy.iterrows():\n",
    "        print(f\"‚Ä¢ Q{row['id']}: {row['question']}\")\n",
    "        print(f\"  Score: {row['factual_accuracy']}/5\")\n",
    "\n",
    "# Find questions with bias concerns\n",
    "low_neutrality = df[df['neutrality'] <= 3]\n",
    "if not low_neutrality.empty:\n",
    "    print(f\"\\n‚ö†Ô∏è  POTENTIAL BIAS CONCERNS ({len(low_neutrality)}):\")\n",
    "    for _, row in low_neutrality.iterrows():\n",
    "        print(f\"‚Ä¢ Q{row['id']}: {row['question']}\")\n",
    "        print(f\"  Neutrality Score: {row['neutrality']}/5\")\n",
    "\n",
    "# Response length analysis\n",
    "print(f\"\\nüìè RESPONSE LENGTH ANALYSIS:\")\n",
    "print(f\"‚Ä¢ Average GPT Response Length: {df['gpt_response_length'].mean():.1f} words\")\n",
    "print(f\"‚Ä¢ Average Ground Truth Length: {df['ground_truth_length'].mean():.1f} words\")\n",
    "print(f\"‚Ä¢ Length Correlation with Quality: {df['gpt_response_length'].corr(df['overall_score']):.3f}\")\n",
    "\n",
    "# Category-specific insights\n",
    "print(f\"\\nüè• CATEGORY-SPECIFIC INSIGHTS:\")\n",
    "for category in df['category'].unique():\n",
    "    cat_data = df[df['category'] == category]\n",
    "    print(f\"\\n{category} ({len(cat_data)} questions):\")\n",
    "    print(f\"  ‚Ä¢ Average Score: {cat_data['overall_score'].mean():.2f}\")\n",
    "    print(f\"  ‚Ä¢ Factual Accuracy: {cat_data['factual_accuracy'].mean():.2f}\")\n",
    "    print(f\"  ‚Ä¢ Neutrality: {cat_data['neutrality'].mean():.2f}\")\n",
    "    \n",
    "    # Find the worst performing question in this category\n",
    "    worst = cat_data.loc[cat_data['overall_score'].idxmin()]\n",
    "    print(f\"  ‚Ä¢ Worst Question: {worst['question'][:50]}... (Score: {worst['overall_score']:.2f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## üìù Export Results for Further Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export analysis results to CSV\n",
    "df_export = df[['id', 'category', 'factual_accuracy', 'clarity', 'neutrality', \n",
    "               'helpfulness', 'overall_score', 'gpt_response_length']].copy()\n",
    "\n",
    "# Save to scores.csv\n",
    "df_export.to_csv('../data/scores.csv', index=False)\n",
    "\n",
    "# Create a summary report\n",
    "with open('../results/analysis_summary.txt', 'w') as f:\n",
    "    f.write(\"LLM HEALTH EVALUATION - ANALYSIS SUMMARY\\n\")\n",
    "    f.write(\"=\"*50 + \"\\n\\n\")\n",
    "    \n",
    "    f.write(f\"Dataset: {len(df)} health questions evaluated\\n\")\n",
    "    f.write(f\"Average Overall Score: {df['overall_score'].mean():.2f}/5.0\\n\\n\")\n",
    "    \n",
    "    f.write(\"SCORES BY CRITERIA:\\n\")\n",
    "    for criterion in ['factual_accuracy', 'clarity', 'neutrality', 'helpfulness']:\n",
    "        f.write(f\"‚Ä¢ {criterion.replace('_', ' ').title()}: {df[criterion].mean():.2f}/5.0\\n\")\n",
    "    \n",
    "    f.write(f\"\\nSCORES BY CATEGORY:\\n\")\n",
    "    for category, score in df.groupby('category')['overall_score'].mean().items():\n",
    "        f.write(f\"‚Ä¢ {category}: {score:.2f}/5.0\\n\")\n",
    "    \n",
    "    f.write(f\"\\nKEY FINDINGS:\\n\")\n",
    "    f.write(f\"‚Ä¢ Questions with concerning factual accuracy (‚â§2): {len(df[df['factual_accuracy'] <= 2])}\\n\")\n",
    "    f.write(f\"‚Ä¢ Questions with potential bias concerns (neutrality ‚â§3): {len(df[df['neutrality'] <= 3])}\\n\")\n",
    "    f.write(f\"‚Ä¢ Average response length: {df['gpt_response_length'].mean():.1f} words\\n\")\n",
    "\n",
    "print(\"‚úÖ Results exported:\")\n",
    "print(\"‚Ä¢ scores.csv - Detailed scores for each question\")\n",
    "print(\"‚Ä¢ analysis_summary.txt - Key findings summary\")\n",
    "print(\"‚Ä¢ overall_performance.png - Main visualization\")\n",
    "\n",
    "print(f\"\\nüéØ MAIN CONCLUSIONS:\")\n",
    "print(f\"‚Ä¢ GPT-4 achieved an average score of {df['overall_score'].mean():.2f}/5.0 across all health questions\")\n",
    "print(f\"‚Ä¢ Strongest area: {['Factual Accuracy', 'Clarity', 'Neutrality', 'Helpfulness'][np.argmax([df[c].mean() for c in criteria])]}\")\n",
    "print(f\"‚Ä¢ Area for improvement: {['Factual Accuracy', 'Clarity', 'Neutrality', 'Helpfulness'][np.argmin([df[c].mean() for c in criteria])]}\")\n",
    "print(f\"‚Ä¢ {len(df[df['factual_accuracy'] <= 2])} questions had concerning factual accuracy scores\")\n",
    "print(f\"‚Ä¢ The model performed best on {df.groupby('category')['overall_score'].mean().idxmax()} questions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
